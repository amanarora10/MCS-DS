---
title: "Assignment1"
author: "Aman Arora (amana4@illinois.edu)"
date: "1/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(class)
library(ggplot2)
set.seed(3402)

```


```{r}
# nc  : number of cluster centers
# sdc : standard deviation for generating the cluster centers
# p: dimension 
generateCenters = function(nc, sdc,p) {

#Centers normally distributed with mean  (1,0)
m1 = matrix(rnorm(csize*p), csize, p)*sigma + cbind( rep(1,csize), rep(0,csize));

#Centers normally distributed with mean  (0,1)
m0 = matrix(rnorm(csize*p), csize, p)*sigma + cbind( rep(0,csize), rep(1,csize));

  return(list("center0" = m0,
              "center1" = m1))
} 
```

```{r}
#Function to generate data
generateData = function(centers, ntr, nte, s, csize) {

# Randomly allocate the n samples for class 1 to the 10 clusters
id1 = sample(1:csize, ntr, replace = TRUE);

# Randomly allocate the n samples for class 1 to the 10 clusters
id0 = sample(1:csize, ntr, replace = TRUE);  

m0 = centers$center0
m1 = centers$center1

#Generate training set
X_train = matrix(rnorm(2*ntr*p), 2*ntr, p)*s + rbind(m1[id1,], m0[id0,])
Y_train = (c(rep(1,ntr), rep(0,ntr)))


train_set = data.frame(X1 = X_train[,1], X2 = X_train[,2], Y = Y_train)

#Generate test set
id1 = sample(1:csize, nte, replace=TRUE);
id0 = sample(1:csize, nte, replace=TRUE); 
X_test = matrix(rnorm(2*nte*p), 2*nte, p)*s +   rbind(m1[id1,], m0[id0,])
Ytest = (c(rep(1,nte), rep(0,nte)))

test_set = data.frame(X1 = X_test[,1], X2 = X_test[,2], Y = Ytest)


return(list("train" = train_set,
              "test" = test_set))
}
```

```{r}
eval_performance  = function(model, data) {

  #Calculate train error
  train_pred = as.integer(model$fitted.values > 0.5)
  train_error = mean(train_pred != data$train$Y )
  
  #Calculate test error
  test_pred = predict(model, newdata = data.frame(data$test[-3])) >0.5
  test_error = mean(test_pred != data$test$Y )
  
  return(list("train_error" = train_error,
              "test_error" = test_error))
}
  
```



```{r}
find_best_k = function(data, foldNum)
{
  dataSet = data$train[, c("X1", "X2", "Y")]  ## 200-by-3 training data
  dataSet$Y = as.factor(dataSet$Y)
  foldSize = floor(nrow(dataSet)/foldNum)  
  KVector = seq(1, (nrow(dataSet) - foldSize), 2)
  min_error = 1
  min_k = 0
  for(K in KVector)
  {
   error = 0
   for(runId in 1:foldNum){
     testSetIndex = ((runId-1)*foldSize + 1):(ifelse(runId == foldNum, nrow(dataSet), runId*foldSize))
     trainX = dataSet[-testSetIndex, c('X1', 'X2')]
     trainY = as.factor(dataSet[-testSetIndex, ]$Y)
     testX = dataSet[testSetIndex, c('X1' , 'X2')]
     testY = as.factor(dataSet[testSetIndex, ]$Y)
     predictY = knn(trainX, testX, trainY, K)
     error = error + sum(predictY != testY) 
   }
   error = error / nrow(dataSet)
   #Check and store the minimum error and corresponding K 
   #the condition '<='  will ensure in case of equal errors higher K will be chosen 
   #since we are going thru K in increasing order. 
   #print(paste("error",error, "min_error",min_error))
   if(error <= min_error)
   {
     min_error = error
     min_k = K
   }

  }
  return(list("min_error" = min_error,
              "best_k" = min_k))
  
}
```

```{r}
mixnorm = function(x, centers0, centers1, s){
  ## return the density ratio for a point x, where each 
  ## density is a mixture of normal with multiple components
  d1 = sum(exp(-apply((t(centers1) - x)^2, 2, sum) / (2 * s^2)))
  d0 = sum(exp(-apply((t(centers0) - x)^2, 2, sum) / (2 * s^2)))
  
  return (d1 / d0)
}
```


#Main function
```{r}
#Generate centers
p = 2;          #dimension    
sigma = 1;      # sd for generating the centers 
csize = 10;       # number of centers
centers = generateCenters(csize,sigma,p)

#Generate data
ntr = 100 #no of training data
nte = 5000 #no of test data
s = sqrt(sigma/5);  # sd for generating x. 
num_sim = 20 #No. of simulations


#Allocate storage for results
linear_train = rep(0,num_sim)
linear_test = rep(0,num_sim)

quad_train = rep(0,num_sim)
quad_test = rep(0,num_sim)

knn_train = rep(0,num_sim)
knn_test = rep(0,num_sim)
knn_best_k = rep(0,num_sim)

bayes_train = rep(0,num_sim)
bayes_test = rep(0,num_sim)

#Repeat simulation num_sim times
for (i in 1:num_sim)  
{  
  data = generateData(centers,ntr,nte,s,csize)
  
  #----------------------------------------------------------------------------------- 
  #Run linear regression 
  model_linear = lm(formula = Y ~ X1 + X2, data = data$train)
  error_linear = eval_performance(model_linear,data)
  linear_train[i] = error_linear$train_error
  linear_test[i] = error_linear$test_error
  
  #----------------------------------------------------------------------------------- 
  #Run quadratic regression 
  model_quadratic = lm(formula = Y ~ X1 + X2 + I(X1 * X2) + I(X1^2) + I(X2^2), data = data$train)
  error_quadratic = eval_performance(model_quadratic,data)
  
  quad_train[i] = error_quadratic$train_error
  quad_test[i] = error_quadratic$test_error
 
  #----------------------------------------------------------------------------------- 
  #KNN classification: find best k
  foldNum = 10
  result= find_best_k(data,foldNum)
  knn_best_k[i] = result$best_k
  
  #KNN Training error
  trainX = data$train[,c('X1','X2')]
  trainY = as.factor(data$train[,'Y'])
  predictY = knn(trainX, trainX, trainY, result$best_k)
  knn_train_error = mean(predictY != trainY )
  knn_train[i] =  knn_train_error

  #KNN Test error
  trainX = data$train[,c('X1','X2')]
  testX = data$test[,c('X1','X2')]
  trainY = as.factor(data$train[,'Y'])
  testY = as.factor(data$test[,'Y'])

  predictY = knn(trainX, testX, trainY, result$best_k)
  knn_test_error = mean(predictY != testY)
  knn_test[i] = knn_test_error 
  
  #----------------------------------------------------------------------------------- 
  #Bayes rule
  
  #Bayes error on training data
  Ytrain_pred_Bayes = apply(trainX, 1, mixnorm, centers$center0,centers$center1,s )
  Ytrain_pred_Bayes = as.numeric(Ytrain_pred_Bayes > 1);
  table(trainY, Ytrain_pred_Bayes); 
  train.err.Bayes = sum(trainY !=  Ytrain_pred_Bayes) / (2*ntr)
  bayes_train[i] = train.err.Bayes
  
  #Bayes error on test data
  Ytest_pred_Bayes = apply(testX, 1, mixnorm, centers$center0,centers$center1,s )
  Ytest_pred_Bayes = as.numeric(Ytest_pred_Bayes > 1);
  table(testY, Ytest_pred_Bayes); 
  test.err.Bayes = sum(testY !=  Ytest_pred_Bayes) / (2*nte)
  bayes_test[i] = test.err.Bayes

}

```

```{r}
n= ntr
traindata = data$train
m0 = centers$center0
m1 = centers$center1

plot(traindata[, 1], traindata[, 2], type = "n", xlab = "", ylab = "")
abline(model_linear)
points(traindata[1:n, 1], traindata[1:n, 2], col = "blue");
points(traindata[(n+1):(2*n), 1], traindata[(n+1):(2*n), 2], col="red"); 

points(m1[1:csize, 1], m1[1:csize, 2], pch="+", cex=1.5, col="blue");    
points(m0[1:csize, 1], m0[1:csize, 2], pch="+", cex=1.5, col="red");   

legend("bottomleft", pch = c(1,1), col = c("red", "blue"), 
       legend = c("class 1", "class 0"))
```



