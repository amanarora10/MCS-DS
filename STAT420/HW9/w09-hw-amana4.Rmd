---
title: "Week 9 - Homework"
author: "Aman Arora (amana4@illinois.edu)"
date: '16th July 2020'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

## Exercise 1 (`longley` Macroeconomic Data)

The built-in dataset `longley` contains macroeconomic data for predicting employment. We will attempt to model the `Employed` variable.

```{r, eval = FALSE}
View(longley)
?longley
```

**(a)** What is the largest correlation between any pair of predictors in the dataset?

```{r}
cor(longley)
```
  
  The population and Year seems to have the largest correlation of 0.9940


**(b)** Fit a model with `Employed` as the response and the remaining variables as predictors. Calculate and report the variance inflation factor (VIF) for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?


```{r}

model1 = lm(Employed ~., longley)
car::vif(model1)

```

The GNP seem to have the largest variance  factor of 1788.513. The VIF also indicates colinearlity since there are more than 2 factor with VIF greater than 5.



**(c)** What proportion of the observed variation in `Population` is explained by a linear relationship with the other predictors?

```{r}
summary(lm(Population ~ . -Employed, longley ))$r.squared

```
  
  About 0.9975 variation in Population is explained by other variables


**(d)** Calculate the partial correlation coefficient for `Population` and `Employed` **with the effects of the other predictors removed**.

```{r}
names(longley)
```


```{r}
x1_model = lm(Population ~. - Employed, longley)
y_model =  lm(Employed ~., longley)
cor(resid(x1_model), resid(y_model)  )
```
  
  * The low value of partial correlation coefficient indicates adding population will not be of much benefit with all other predictors present. 

**(e)** Fit a new model with `Employed` as the response and the predictors from the model in **(b)** that were significant. (Use $\alpha = 0.05$.) Calculate and report the variance inflation factor for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?


```{r}
summary(model1)
```


```{r}
new_model  = lm(Employed ~ Unemployed   + Armed.Forces + Year, longley)
car::vif(new_model)

```

  * The year has the largest VIF but since all of them are less than 5 they do not indicate  any multicolinearlity.  


**(f)** Use an $F$-test to compare the models in parts **(b)** and **(e)**. Report the following:

- The null hypothesis
- The test statistic
- The distribution of the test statistic under the null hypothesis
- The p-value
- A decision
- Which model you prefer, **(b)** or **(e)**

  
  
```{r}
 anova(new_model,model1) 

```
  
  * The $H_0$ is the $\beta$s of GNP.deflator, GNP, and Population are all 0.
  * The $H_1$ at least one of  $\beta$s of GNP.deflator, GNP, and Population is $\neq$ 0.
  * The test statistic under the null hypothesis has $F_{(3,9)}$ distribution 
  * The p-value is 0.23
  * with $\alpha$ = .95 we fail to reject NULL hypothesis
  * Thus the model from  **(e)** is preferable 


**(g)** Check the assumptions of the model chosen in part **(f)**. Do any assumptions appear to be violated?

```{r, echo = FALSE}
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}
```


```{r}
plot_fitted_resid(new_model)
plot_qq(new_model)
```
  
* Based on the plots the normal distribution of residuals (Q-Q plot) and constant variance assumption (residuals vs fitted plot) looks reasonable. 


***

## Exercise 2 (`Credit` Data)

For this exercise, use the `Credit` data from the `ISLR` package. Use the following code to remove the `ID` variable which is not useful for modeling.

```{r}
library(ISLR)
data(Credit)
Credit = subset(Credit, select = -c(ID))
```

Use `?Credit` to learn about this dataset.

**(a)** Find a "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `140`
- Obtain an adjusted $R^2$ above `0.90`
- Fail to reject the Breusch-Pagan test with an $\alpha$ of $0.01$
- Use fewer than 10 $\beta$ parameters

Store your model in a variable called `mod_a`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.


```{r}
#Remove rating due to high VIF
mod_a = lm(Balance ~  log(Income) + Limit +  Ethnicity + Student , Credit)

#Find indices of rows that do not have influential observations
retain_idx =  cooks.distance(mod_a) < 4/length(cooks.distance(mod_a))

#Fit a new model without influential observations 
mod_a = lm(Balance ~  log(Income) + Limit +  Ethnicity +Student   , data = Credit[retain_idx,])

#Ensure VIF is below 5
car::vif(mod_a)
```



```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```


```{r, eval = TRUE}
get_loocv_rmse(mod_a)
get_adj_r2(mod_a)
get_bp_decision(mod_a, alpha = 0.01)
get_num_params(mod_a)

```
  
    
* The model seems to be meeting the below criteria:

- Reach a LOOCV-RMSE below `140`
- Obtain an adjusted $R^2$ above `0.90`
- Fail to reject the Breusch-Pagan test with an $\alpha$ of $0.01$
- Use fewer than 10 $\beta$ parameters

**(b)** Find another "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `130`
- Obtain an adjusted $R^2$ above `0.85`
- Fail to reject the Shapiro-Wilk test with an $\alpha$ of $0.01$
- Use fewer than 25 $\beta$ parameters

Store your model in a variable called `mod_b`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.

```{r}
pairs(Credit[retain_idx,])
```


```{r}
#Remove rating due to high VIF
mod_b = lm(Balance ~  log(Income) + (Limit*Age*Cards) + I(Limit^2) + I(Age^2) + I(log(Income) ^2)  + Ethnicity + Student + Married , Credit)

#Find indices of rows that do not have influential observations
retain_idx =  cooks.distance(mod_a) < 4/length(cooks.distance(mod_a))

#Fit a new model without influential observations 
mod_b = lm(Balance ~  log(Income) + (Limit*Age*Cards) + I(Limit^2) + I(Age^2) + I(log(Income) ^2)  + Ethnicity + Student + Married  , data = Credit[retain_idx,])

```


```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

```{r, eval = TRUE}
get_loocv_rmse(mod_b)
get_adj_r2(mod_b)
get_sw_decision(mod_b, alpha = 0.01)
get_num_params(mod_b)
```
* The model seems to be meeting the below criteria:

- Reach a LOOCV-RMSE below `130`
- Obtain an adjusted $R^2$ above `0.85`
- Fail to reject the Shapiro-Wilk test with an $\alpha$ of $0.01$
- Use fewer than 25 $\beta$ parameters
***

## Exercise 3 (`Sacramento` Housing Data)

For this exercise, use the `Sacramento` data from the `caret` package. Use the following code to perform some preprocessing of the data.

```{r}
library(caret)
library(ggplot2)
data(Sacramento)
sac_data = Sacramento
sac_data$limits = factor(ifelse(sac_data$city == "SACRAMENTO", "in", "out"))
sac_data = subset(sac_data, select = -c(city, zip))
```

Instead of using the `city` or `zip` variables that exist in the dataset, we will simply create a variable (`limits`) indicating whether or not a house is technically within the city limits of Sacramento. (We do this because they would both be factor variables with a **large** number of levels. This is a choice that is made due to laziness, not necessarily because it is justified. Think about what issues these variables might cause.)

Use `?Sacramento` to learn more about this dataset.

A plot of longitude versus latitude gives us a sense of where the city limits are.

```{r}
qplot(y = longitude, x = latitude, data = sac_data,
      col = limits, main = "Sacramento City Limits ")
```

After these modifications, we test-train split the data.

```{r}
set.seed(420)
sac_trn_idx  = sample(nrow(sac_data), size = trunc(0.80 * nrow(sac_data)))
sac_trn_data = sac_data[sac_trn_idx, ]
sac_tst_data = sac_data[-sac_trn_idx, ]
```

The training data should be used for all model fitting. Our goal is to find a model that is useful for predicting home prices.

**(a)** Find a "good" model for `price`. Use any methods seen in class. The model should reach a LOOCV-RMSE below 77,500 in the training data. Do not use any transformations of the response variable.


```{r}
full_model = lm(price ~ ., data = sac_trn_data )

# Do a backward search using AIC
housing_model = step(full_model, direction = "backward")

```

```{r}
#use the model with best AIC and Check the RMSE of this model
get_loocv_rmse(housing_model)
```

**(b)** Is a model that achieves a LOOCV-RMSE below 77,500 useful in this case? That is, is an average error of 77,500 low enough when predicting home prices? To further investigate, use the held-out test data and your model from part **(a)** to do two things:

- Calculate the average percent error:
\[
\frac{1}{n}\sum_i\frac{|\text{predicted}_i - \text{actual}_i|}{\text{predicted}_i} \times 100
\]
- Plot the predicted versus the actual values and add the line $y = x$.

Based on all of this information, argue whether or not this model is useful.


```{r}

predicted = predict(housing_model, newdata = sac_tst_data)

acc_error = 0
for (i in 1: length(predicted))
{
  error = abs(predicted[i] - sac_tst_data[i,"price"])/predicted[i] 
  acc_error = acc_error + error
}

avg_error = (acc_error/length(predicted))*100
avg_error
```
  
  The average error is about `r avg_error` %

```{r}
plot(sac_tst_data[,"price"], predicted, ylab = "Predicted Price", xlab = "Actual price")
grid()
abline(a = 0 , b = 1, col = "red")
```
  
  * Based on the plot of predicted vs actual values and % error the model seems to be making reasonably correct prediction for "actual prices" below 600K. Above 600K actual price the error seems to be quite large. The model  thus has utility for making predictions below actual price of 600K.


***

## Exercise 4 (Does It Work?)

In this exercise, we will investigate how well backwards AIC and BIC actually perform. For either to be "working" correctly, they should result in a low number of both **false positives** and **false negatives**. In model selection,

- **False Positive**, FP: Incorrectly including a variable in the model. Including a *non-significant* variable
- **False Negative**, FN: Incorrectly excluding a variable in the model. Excluding a *significant* variable

Consider the **true** model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_5 + \beta_6 x_6 + \beta_7 x_7 + \beta_8 x_8 + \beta_9 x_9 + \beta_{10} x_{10} + \epsilon
\]

where $\epsilon \sim N(0, \sigma^2 = 4)$. The true values of the $\beta$ parameters are given in the `R` code below.

```{r}
library(knitr)
beta_0  = 1
beta_1  = -1
beta_2  = 2
beta_3  = -2
beta_4  = 1
beta_5  = 1
beta_6  = 0
beta_7  = 0
beta_8  = 0
beta_9  = 0
beta_10 = 0
sigma = 2
```

Then, as we have specified them, some variables are significant, and some are not. We store their names in `R` variables for use later.

```{r}
not_sig  = c("x_6", "x_7", "x_8", "x_9", "x_10")
signif = c("x_1", "x_2", "x_3", "x_4", "x_5")
```

We now simulate values for these `x` variables, which we will use throughout part **(a)**.

```{r}
set.seed(420)
n = 100
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = runif(n, 0, 10)
x_9  = runif(n, 0, 10)
x_10 = runif(n, 0, 10)
```

We then combine these into a data frame and simulate `y` according to the true model.

```{r}
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```

We do a quick check to make sure everything looks correct.

```{r}
head(sim_data_1)
```

Now, we fit an incorrect model.

```{r}
fit = lm(y ~ x_1 + x_2 + x_6 + x_7, data = sim_data_1)
coef(fit)
```

Notice, we have coefficients for `x_1`, `x_2`, `x_6`, and `x_7`. This means that `x_6` and `x_7` are false positives, while `x_3`, `x_4`, and `x_5` are false negatives.

To detect the false negatives, use:

```{r}
# which are false negatives?
!(signif %in% names(coef(fit)))
```

To detect the false positives, use:

```{r}
# which are false positives?
names(coef(fit)) %in% not_sig
```

Note that in both cases, you could `sum()` the result to obtain the number of false negatives or positives.

**(a)** Set a seed equal to your birthday; then, using the given data for each `x` variable above in `sim_data_1`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table.


```{r}
n_sim = 300
set.seed(19771212)

fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)

#No of real predictors
num_real_pred = 5

for (i in 1:n_sim)
{
  
  #Simulate to generate y with noise
  sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma))
  
  #Fit a addtive model with all predictors
  model1 = lm(y~.,sim_data_1)
  
  #Select best based on AIC
  fit = step(model1, direction = "backward", trace = FALSE)
  #False negative
  fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
  #False positive
  fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
  
  #Select best based on BIC
  fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
  #False negative
  fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
  #False positive
  fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
  
}

#Format the results for display in a table
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))

final_result = data.frame(rbind(aic_result, bic_result) )
names(final_result) = c("False Negative", "False Positive")
row.names(final_result) = c("AIC","BIC")

```
`r kable(final_result,"markdown",caption ="Model Comparison")`
  
  *Both AIC/BIC metric are able to pick correct predictors consistently although they pick incorrect predictors as well with small probability. Also BIC seems to have lower false positive rate than AIC metric. 

**(b)** Set a seed equal to your birthday; then, using the given data for each `x` variable below in `sim_data_2`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table. Also compare to your answers in part **(a)** and suggest a reason for any differences.

```{r}
set.seed(94)
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = x_1 + rnorm(n, 0, 0.1)
x_9  = x_1 + rnorm(n, 0, 0.1)
x_10 = x_2 + rnorm(n, 0, 0.1)

sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```


```{r}
n_sim = 300
set.seed(19771212)

fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)

#No of real predictors
num_real_pred = 5

for (i in 1:n_sim)
{
  
  #Simulate to generate y with noise
sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma))
  
  #Fit a additive model with all predictors
  model1 = lm(y~.,sim_data_2)
  
  #Select best based on AIC
  fit = step(model1, direction = "backward", trace = FALSE)
  #False negative
  fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
  #False positive
  fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
  
  #Select best based on BIC
  fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
  #False negative
  fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
  #False positive
  fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
  
}

#Format the results for display in a table
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))

final_result = data.frame(rbind(aic_result, bic_result) )
names(final_result) = c("False Negative", "False Positive")
row.names(final_result) = c("AIC","BIC")
final_result
```
`r kable(final_result,"markdown",caption ="Model Comparison")` 

 * With additions of predictors variables that  have correlation  with the actual model predictors  is resulting in increased false positive and false negative rates vs previous in part **(a)**. The AIC and BIC metrics are now less capable  to detect the actual predictors due to this correlation as we see from the plots below x1 has linear relationship between x8 and x9 while x10 seems to have a linear relationship with x2.
 
* The backward search is  also  picking the incorrect predictors are at a higher rate due to linear relationship  between real and incorrect predictors.
 
```{r}
pairs(sim_data_2)
```
 




