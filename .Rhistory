print(result)
}
n_sim = 1
set.seed(19771212)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
model1 = lm(y~.,sim_data_1)
fit = step(model1, direction = "backward", trace = FALSE)
result = !(signif %in% names(coef(fit)))
print(fit$terms)
print(result)
}
n_sim = 1
set.seed(19771212)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
model1 = lm(y~.,sim_data_1)
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
result = !(signif %in% names(coef(fit)))
print(fit$terms)
print(result)
#False postitive
result = names(coef(fit)) %in% not_sig
print(fit$terms)
print(result)
}
n_sim = 1
set.seed(19771212)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_aic[i] = names(coef(fit)) %in% not_sig
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count[i] = names(coef(fit)) %in% not_sig
}
n_sim = 1
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_aic[i] = names(coef(fit)) %in% not_sig
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_bic[i] = names(coef(fit)) %in% not_sig
}
n_sim = 1
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_aic[i] = names(coef(fit)) %in% not_sig
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)
}
n_sim = 1
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_aic[i] = names(coef(fit)) %in% not_sig
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
#fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
#fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)
}
n_sim = 1
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)
}
n_sim = 300
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)
}
n_sim = 300
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)
}
n_sim = 300
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)
}
result_df = data.frame(rbind(fn_count_aic,fp_count_aic, fn_count_bic,fp_count_bic ))
View(result_df)
n_sim = 300
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))
#False positive
fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)
}
result_df = data.frame(cbind(fn_count_aic,fp_count_aic, fn_count_bic,fp_count_bic ))
View(result_df)
View(result_df)
result_df = data.frame(cbind(fn_count_aic,fp_count_aic, fn_count_bic,fp_count_bic ))
mean(result_df)
result_df = data.frame(cbind(fn_count_aic,fp_count_aic, fn_count_bic,fp_count_bic ))
result_df
n_sim = 300
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
#No of real predictors
num_real_pred = 5
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a addtive model with all predictors
model1 = lm(y~.,sim_data_1)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
#False positive
fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
#False positive
fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
}
result_df = data.frame(cbind(fn_count_aic,fp_count_aic, fn_count_bic,fp_count_bic ))
result_df = data.frame(cbind(fn_count_aic,fp_count_aic, fn_count_bic,fp_count_bic ))
result_df
result_df = data.frame(cbind(mean(fn_count_aic),mean(fp_count_aic), mean(fn_count_bic),mean(fp_count_bic) ))
result_df
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
aic_result
data.frame(cbind(mean(fn_count_aic),mean(fp_count_aic), mean(fn_count_bic),mean(fp_count_bic) ))
result_df
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))
final_result = rbind(aic_result, bic_result)
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))
final_result = rbind(aic_result, bic_result)
final_result
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))
final_result = data.frame(rbind(aic_result, bic_result), )
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))
final_result = data.frame(rbind(aic_result, bic_result) )
names(final_result) = c("False Negative", "False Positive")
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))
final_result = data.frame(rbind(aic_result, bic_result) )
names(final_result) = c("False Negative", "False Positive")
final_result
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))
final_result = data.frame(rbind(aic_result, bic_result) )
names(final_result) = c("False Negative", "False Positive")
row.names(final_result) = c("AIC","BIC")
final_result
n_sim = 300
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
#No of real predictors
num_real_pred = 5
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a additive model with all predictors
model1 = lm(y~.,sim_data_2)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
#False positive
fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
#False positive
fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
}
#Format the results for display in a table
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))
final_result = data.frame(rbind(aic_result, bic_result) )
names(final_result) = c("False Negative", "False Positive")
row.names(final_result) = c("AIC","BIC")
final_result
set.seed(94)
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = x_1 + rnorm(n, 0, 0.1)
x_9  = x_1 + rnorm(n, 0, 0.1)
x_10 = x_2 + rnorm(n, 0, 0.1)
sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma)
)
n_sim = 300
set.seed(19771212)
fn_count_aic = rep(0,n_sim)
fp_count_aic = rep(0,n_sim)
fn_count_bic = rep(0,n_sim)
fp_count_bic = rep(0,n_sim)
#No of real predictors
num_real_pred = 5
for (i in 1:n_sim)
{
#Simulate to generate y with noise
sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 +
beta_5 * x_5 + rnorm(n, 0 , sigma))
#Fit a additive model with all predictors
model1 = lm(y~.,sim_data_2)
#Select best based on AIC
fit = step(model1, direction = "backward", trace = FALSE)
#False negative
fn_count_aic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
#False positive
fp_count_aic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
#Select best based on BIC
fit = step(model1, direction = "backward", trace = FALSE , k = log(n))
#False negative
fn_count_bic[i] = sum(!(signif %in% names(coef(fit))))/num_real_pred
#False positive
fp_count_bic[i] = sum(names(coef(fit)) %in% not_sig)/num_real_pred
}
#Format the results for display in a table
aic_result = cbind(mean(fn_count_aic),mean(fp_count_aic))
bic_result = cbind(mean(fn_count_bic),mean(fp_count_bic))
final_result = data.frame(rbind(aic_result, bic_result) )
names(final_result) = c("False Negative", "False Positive")
row.names(final_result) = c("AIC","BIC")
final_result
library(knitr)
beta_0  = 1
beta_1  = -1
beta_2  = 2
beta_3  = -2
beta_4  = 1
beta_5  = 1
beta_6  = 0
beta_7  = 0
beta_8  = 0
beta_9  = 0
beta_10 = 0
sigma = 2
pairs(sim_data_2)
exp(1)/1+exp(1)
exp(1)/(1+exp(1))
exp(-2.75)/(1+exp(-2.75))
-3+.75
exp(-2.25)/(1+exp(-2.25))
head(mtcars)
model_fit = glm(am ~ mpg + hp+ qsec, data = mtcars,family = binomial )
model_fit = glm(am ~ mpg + hp+ qsec, data = mtcars,family = binomial )
summary(model_fit)
?mtcars
new_data = data.frame(mpg = 19, hp = 150, qsec = 19 )
predict(model_fit, newdata = new_data)
new_data = data.frame(mpg = 19, hp = 150, qsec = 19 )
predict(model_fit, newdata = new_data, type = "response")
new_data = data.frame(mpg = 19, hp = 150, qsec = 19 )
predict(model_fit, newdata = new_data, type = "link")
new_data = data.frame(mpg = 19, hp = 150, qsec = 19 )
predict(model_fit, newdata = new_data)
new_data = data.frame(mpg = 22, hp = 123, qsec = 18 )
predict(model_fit, newdata = new_data, type = "response")
summary(model_fit)
model_null = glm(am ~ 1, data = mtcars,family = binomial )
model_null = glm(am ~ 1, data = mtcars,family = binomial )
anova(model_null, model_fit, test = "LRT")
summary(model_fit)
?MASS::Pima.tr
is.factor(Pima.tr$type)
?MASS::Pima.tr
library(MASS)
is.factor(Pima.tr$type)
model_fit = glm(type ~ I((glu*ped)^2), data = Pima.tr, family = binomial)
summary(model_fit)
model_fit = glm(type ~ glu*ped + I(glu^2) + I (ped^2), data = Pima.tr, family = binomial)
summary(model_fit)
predict(model_fit, newdata = Pima.te, type = "response")
mean(predict(model_fit, newdata = Pima.te, type = "response") > .8)
model_fit = glm(type ~.*., data = Pima.tr, family = binomial )
summary(model_fit)
library(MASS)
library(MASS)
model_add = glm(type ~ ., data = Pima.tr, family = binomial)
predicted  =  ifelse (predict(model_add, Pima.te) > 0,"Yes","No")
P = sum(Pima.te$type == "Yes")
(sensitivity = TP/P)
library(MASS)
library(MASS)
model_add = glm(type ~ ., data = Pima.tr, family = binomial)
predicted  =  predict(model_add, Pima.te) > 0
P = sum(Pima.te$type == "Yes")
p = exp(-2.25)/(1+exp(-2.25))
p = exp(-2.25)/(1+exp(-2.25))
p
p = exp(-2.25)/(1+exp(-2.25))
1-p
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
sample_size = 150
set.seed(120)
x1 = rnorm(n = sample_size)
x2 = rnorm(n = sample_size)
x3 = rnorm(n = sample_size)
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
library(boot)
(result_add= suppressWarnings(cv.glm(df_train, model_add_partial, K = 5)$delta[1]))
?cooks.distance
