---
title: "Box Office Blues"
author: "STAT-420, Team: Summer Proj, A Arora, S Dani, G Shrivastava"
date: 'July 2020'
output:
  html_document:
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")

cbPalette = c("orange2", "darkturquoise", "hotpink", "green", "royalblue", "darkgreen", "tomato", "lightseagreen")

```


# Introduction
In 2018, the global box office was worth \$41.7 billion. In 2019, total earnings at the North American box office amounted to \$11.32 billion. The magic movies create in our daily lives is undeniable, but more interesting to us is the story the data tells us.

In this work, we build multiple linear regression models to predict the **Revenue** of a movie, given its attributes. Different features like 'genre', 'runtime', 'budget', 'vote_average', 'vote_count','production_companies' and their interactions and higher order terms are explored to find the best model for revenue prediction. 

We follow a systematic process of model selection. Starting with an additive linear regression model of the form 
\[
Y_i = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \epsilon
\]  
we work our way quickly through full additive models, log responses, power response, interaction terms and polynomial predictors. We use stepwise approaches and exhaustive search to find the best models for each approach. Model assumptions of normal distribution and constant variance are also tested. The following values are recorded for the models under consideration:

- **loocv_rmse** or cross-validated RMSE as a measure of generalization of the model.
- **Adjusted $R^2$** as a measure of the explainability of revenue with the chosen predictors.
- **Predictor Count** as a measure of model complexity.
- **Normality Assumption**: The Breusch-Pagan test statistic and decision given a significance level of $\alpha = 0.01$.
- **Equal Variance Assumption**: The Shapiro-Wilk test statistic and decision given a significance level of $\alpha = 0.01$.
- **Average Percent Error**: Our final measure of model success is obtained by validating model performance on a test dataset.


## Dataset
Our search for high quality movie metadata led us to the [TMDB 5000 Movie Dataset](https://www.kaggle.com/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv) provided by Kaggle. This dataset contains metadata and revenue information for over 5000 movies. A few  variables of interest are:

-  **Original_title**: Name of the movie
-  **Budget**: Budget of movies in USD (numeric)
-  **Revenue**: Revenue of movie in USD (numeric)
-  **Original Language**: The language in which movie was originally produced (factor variable)
-  **Genres**: Genre of the movie (factor variable)
-  **Popularity**: A numeric metric to measure popularity of the movie (numeric)
-  **Vote Average**: A numeric metric to measure average vote from audience (numeric)
-  **Runtime** : A numeric metric for the total runtime(in min) of the movie (numeric)
-  **Production Companies** : A categorical for the production companies name  (factor)
  
The TMDB 5000 Movie Dataset contains two csv files - one related to movies and the other on movie credits. While both containing interesting information that could impact revenue, this project's focus was on the attributes contained in [`tmdb_5000_movies.csv`](tmdb_5000_movies.csv). This data file is a csv file with 4803 records and 20 columns and formed the basis for the entirety of this study. 


****

```{r echo = FALSE, message = FALSE, warning = FALSE}
library('readr')
library(broom)
library(tidyverse)
library(kableExtra)
library(faraway)
library(leaps)
library(flextable)
library(pagedown)
library(tm)
library(data.table)
```

# Methods

```{r message = FALSE, warning = FALSE}
tmdb_movies = read.csv("tmdb_5000_movies.csv" , stringsAsFactors = FALSE)

```

As a first step, we create a data frame from the tmdb_5000_movies.csv file. This data frame contains `r length(tmdb_movies)` attributes of `r nrow(tmdb_movies)` movies.

## Data Preparation
Before creating the models, we go through a process of data analysis, transformation and cleaning. In this step, tmdb_movies is transformed to extract relevant data points which could be useful for model building. Some of the tenets we adopted during this process:

- **Value Distribution**: To be considered as a predictor, the attribute's value range should have reasonable coverage within the dataset.
- **Numeric vs Categorical Variables**: Consider predictor count inflation while deciding whether a predictor should be modeled as numeric or categorical.
- **Text Content**: While movie overview, keywords and other textual content in movie metadata could be interesting predictors if we used NLP techniques, we drop unique content attributes while building the linear regression models. 

```{r message = FALSE, warning = FALSE}
#----DATA Analysis: original_language ----#

#retain top 5 languages; else make it "not"
top_5_lang = tmdb_movies %>%
  group_by(original_language) %>%
  tally(sort = T) %>%
  ungroup() %>%
  arrange(desc(n))
top_5_lang = head(top_5_lang, 5)

tmdb_movies$top_5_lang = ifelse(tmdb_movies$original_language %in% 
                                  top_5_lang$original_language, 
                                 tmdb_movies$original_language, 
                                 "not")

en_percent = mean(tmdb_movies$top_5_lang == 'en') * 100
top_5_lang_percent = mean(tmdb_movies$top_5_lang != 'not') * 100

```

```{r message = FALSE, warning = FALSE}
#----DATA Cleaning and Transformation: release_date ----#

#Break up date into month and year
tmdb_movies$release_month=format(as.Date(tmdb_movies$release_date), "%m")
tmdb_movies$release_year=format(as.Date(tmdb_movies$release_date), "%Y")

```

```{r message = FALSE, warning = FALSE}

#----DATA Cleaning and Transformation: genres ----#

#Get the first genre of the movie. Typically a movie is a combination of many genres, but our dataset represents the most weighted genre as the first genre. We extract this so that we have a 1:1 mapping of a movie to its top genre.

tmdb_movies$first_genre=as.numeric(gsub(",","",substr(tmdb_movies$genres,9, regexpr(",", tmdb_movies$genres))))

```

```{r message = FALSE, warning = FALSE}
#----DATA Cleaning and Transformation: production_companies ----#

#remove punctuation
tmdb_movies$clean_company = removePunctuation(tmdb_movies$production_companies)
#remove spaces
tmdb_movies$clean_company = tolower(gsub("[[:blank:]]", "", tmdb_movies$clean_company))

###  variable creation #####
company_rnk = read.csv("company_ranking.csv" , stringsAsFactors = FALSE)

#remove punctuation
company_rnk$clean_company = removePunctuation(company_rnk$Production_company)
#remove spaces and lower case for each of mathching
company_rnk$clean_company = tolower(gsub("[[:blank:]]", "", company_rnk$clean_company))

#take top 10 as Big Banners
top_10 = head(company_rnk,n=10)
top_10_list = paste(unique(top_10$clean_company), collapse = '|')

#Create a new variable 'is_big_banner' which is dummy variable to indicate if the movie was produced by a Top 10 production  companies'
#tmdb_movies$is_big_banner <- as.integer(as.logical(tmdb_movies$clean_company %like% top_10_list))
tmdb_movies$is_big_banner <- tmdb_movies$clean_company %like% top_10_list

```

The following columns are of specific interest: 

- **original_language**: The data contained in tmdb_movies is very skewed towards English. `r en_percent`% mvoies in the dataset are in English and `r top_5_lang_percent`% movies are in the top 5 languages. We drop original_language as a predictor of revenue because of the nature of its distribution.
- **release_date**: The month and year of the release date are extracted as two separate columns. This will allow us to test month independently as a predictor (e.g. revenue of summer movies or movies released during holiday season) versus the year of release.
- **genres**: The tmdb_movies data contains 'genres' variable as a key:value pair. To make it more useful as a predictor, transformation is applied to extract the first genre from the list. Based on visual scan of the data, it is apparent that the first genre in list is predominately the major genre of the movie. We add a new variable 'first_genre' to the tmdb_movies data frame. first_genre is later set as a categorical variable with genre id as the value.
- **production_companies**: Basic data cleaning tasks are performed with standardization of production companies in mind. Special characters are removed and the string is converted to uppercase. This step is a precursor to the creation of a custom variable **is_banner_flag** which will be set if the production company is a top 10 production company. We use data from [Movie Production Companies](https://www.the-numbers.com/movies/production-companies/) to create a reference table for our lookups to determine whether a production company is in the top 10 or not. Please refer to [company_ranking.csv](company_ranking.csv) for details.


A new data frame **tmdb_movies_small** is created with just the columns of interest. This data frame is used for further exploration and model building.

```{r message = FALSE, warning = FALSE}
#Select the relevant columns
col_sel = c( "original_title","revenue", "budget","popularity", 
             "vote_average","vote_count", "runtime",
             "release_month", "release_year", 
             "first_genre", "is_big_banner")

tmdb_movies_small = tmdb_movies[,col_sel]
```

## Exploratory Analysis
Before creating the models, we go through a process of data exploration with tmdb_movies_small to understand the value range of various features, their relationships with each other and with Revenue, our target variable. 

Here is a snippet of the data with the columns under consideration:

```{r message = FALSE, warning = FALSE}
ft <- flextable(head(tmdb_movies_small, n=10))
ft <- autofit(ft)
ft

```

Following the tenets outlined earlier, we make the following decisions:

- **release_month**: We treat release_month as a categorical variable. This will help us analyze revenue outcomes by month.
- **release_year**: tmdb_movies_small contains movies release between `r min(tmdb_movies_small$release_year)` and `r max(tmdb_movies_small$release_year)`. While release_year can be treated as a categorical ordinal variable, we decide to treat it as an integer to reduce model complexity.
- **first_genre**: we treat this as a categorical variable with `r length(unique(tmdb_movies_small$first_genre))` distinct values in its value set.
- **is_big_banner**: we treat this as a categorical variable with values {true, false}

```{r message = FALSE, warning = FALSE}
#Find rows with 0 values and set to NA
tmdb_movies_small[tmdb_movies_small$revenue == 0, "revenue" ] = NA
tmdb_movies_small[tmdb_movies_small$budget == 0, "budget" ] = NA
tmdb_movies_small[tmdb_movies_small$popularity == 0, "popularity" ] = NA
#tmdb_movies_small[tmdb_movies_small$vote_count == 0, "vote_average" ] = NA

tmdb_movies_small$release_year = as.integer(tmdb_movies_small$release_year)
tmdb_movies_small$release_month = as.factor(tmdb_movies_small$release_month)
tmdb_movies_small$first_genre = as.factor(tmdb_movies_small$first_genre)
tmdb_movies_small$is_big_banner = as.factor(tmdb_movies_small$is_big_banner)

#remove invalid rows
tmdb_movies_small =  na.omit(tmdb_movies_small)

#dropping original_title as we build the models. Retaining tmdb_movies_small so we can use original_title during results analysis
tmdb_movies_small_no_title = subset(tmdb_movies_small, select = -c(original_title))

```

We drop the movie title column from tmdb_movies_small. If needed, we will look at the title later during results analysis.

```{r fig.width=12, fig.height=12, message = FALSE, warning = FALSE}
pairs(tmdb_movies_small_no_title, col = cbPalette[2])
```

TODO: Write up analysis of the relationships. @aman/@gaurav, can one of you pick this up? This is required to complete the Exploratory Analysis section


## Test-Train Split

```{r message = FALSE, warning = FALSE}
set.seed(420)
train_idx  = sample(nrow(tmdb_movies_small_no_title), size = trunc(0.80 * nrow(tmdb_movies_small_no_title)))

tmdb_movies_train = tmdb_movies_small_no_title[train_idx, ]
tmdb_movies_test = tmdb_movies_small_no_title[-train_idx, ]
```

We withold a validation set so that we can assess model performance. We split our tmdb_movies_small dataset randomly into 2 parts - train (80% ) and test(20%).

- **Train Dataset**: The train dataset is used to train the models and also create a leave-oneout cross-validated RMSE (loocv_rmse). By creating RMSE scores for different sets created by leaving one observation out, we obtain a measure that can be used to assess how the model will generalize. Lower the loocv_rmse, better the model performance against unseen data.

- **Test Dataset**: We run every model on the test dataset and obtain the average percent error as a measure of model generalization.

### Issue: Unseen Factor Levels

Because of the test-train split, we encounter a new problem with respect to categorical variables during model building. If the test dataset contains unseen values for a factor variable, the model would output errors while trying to predict revenue for the test dataset. Our approach is to drop those rows from test set before we begin modeling.

```{r message = FALSE, warning = FALSE}
# We need to make sure our training set has all the values for the factor variables, else when we call predict on the test data set, we get errors. Our approach is to drop those rows from test set before we begin modeling.

drop_new_factor_levels = function(i) {
   train_i = tmdb_movies_train[, i]
   test_i = tmdb_movies_test[, i]

   diff = (unique(test_i) %in% unique(train_i))
   if (is.factor(test_i) & any(!diff)) {
      test_i_levels = unique(test_i)
      apply(test_i ==
              matrix(rep(test_i_levels[diff], each = nrow(tmdb_movies_test)),
                     nrow = nrow(tmdb_movies_test)),
            1,
            any)
   } else {
      rep(TRUE, nrow(tmdb_movies_test))
   }
}

keep = apply(sapply(1:ncol(tmdb_movies_test),
                    drop_new_factor_levels),
             1,
             all)
tmdb_movies_test = tmdb_movies_test[keep, ]

```


## Model Building: Support Methods
To create a structured approach to the model building process, we implemented a number of support methods:

- **get_bp_decision**: Given a model and significance level, make a decision on whether the model rejects or fails to reject the normality assumption.
- **get_sw_decision**: Given a model and significance level, make a decision on whether the model rejects or fails to reject the constant variance assumption.
- **get_num_params**: Given a model, returns how many coefficients the model has.
- **get_loocv_rmse**: Given a model, obtain the loocv_rmse.
- **get_adj_r2**: Return the adjusted $R^2$ of a model.
- **get_model_results**: Return a list all the results we need to evaluate a model
- **pasteFormula**: return a display string that consolidates the model parameters.
- **removeInfluencers**: Use Cook's distance and normal evaluation criteria to drop influential points.


```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

get_model_results = function(model, alpha = 0.01) {
  model_str = as.character(as.formula(model))[3]
  loocv_rmse = get_loocv_rmse(model)
  adj_r2 = get_adj_r2(model)
  bp_decision = get_bp_decision(model, alpha)
  bp_value = bptest(model)$p.value
  sw_decision = get_sw_decision(model, alpha)
  sw_value = shapiro.test(resid(model))$p.value
  num_params = get_num_params(model)

  list(
       model = model_str,
       loocv_rmse = loocv_rmse,
       adj_r2 = adj_r2,
       bp_decision = bp_decision,
       bp_value = bp_value,
       sw_decision = sw_decision,
       sw_value = sw_value,
       num_params = num_params)
}

pasteFormula = function(outcome, variables) {
  pastef = ""
  if (!is.null(outcome)) {
    pastef = paste(outcome, paste(variables, collapse = " + "), sep = " ~ ")
  } else {
    pastef = paste(" ~ ", paste(variables, collapse = " + "))
  }
  return (pastef)
}

removeInfluencers = function(data) {
  model = lm(Revenue ~ ., data=data)
  cd_mod_a = cooks.distance(model)
  influential_ind = cd_mod_a > 4 / length(cd_mod_a)
  data_modified = data[influential_ind == FALSE, ]
  return (data_modified)
}

```

```{r message = FALSE, warning = FALSE}
revenue_mod_results = tribble(~model_name, ~model, ~loocv_rmse, ~adj_r2, ~sw_value, ~sw_decision, ~bp_value, ~bp_decision, ~num_params)
```


## Model Creation
```{r message = FALSE, warning = FALSE}
#Fit a simple  model and print summary
boxoffice_model_1 = lm(revenue ~  budget + popularity , tmdb_movies_small)
summary(boxoffice_model_1)

results_simple = get_model_results(boxoffice_model_1)
revenue_mod_results = rbind(revenue_mod_results,
                          (tibble_row(model_name = "simple",
                                      model = results_simple$model,
                                      loocv_rmse = results_simple$loocv_rmse,
                                      adj_r2 = results_simple$adj_r2,
                                      sw_value = results_simple$sw_value,
                                      sw_decision = results_simple$sw_decision,
                                      bp_value = results_simple$bp_value,
                                      bp_decision = results_simple$bp_decision,
                                      num_params = results_simple$num_params
                                      )))


```


A simple additive model above was used to predict revenue based on popularity and budget of the movie. The predictors seems to be significant and with Adjusted R-squared been 0.6062.Our objective is to improve this simplistic model and find the best performing model with as the preferred model for this project.


```{r}
fit_revenue_add = lm(revenue ~ .,
                     data = tmdb_movies_train)

results_add = get_model_results(fit_revenue_add)
revenue_mod_results = rbind(revenue_mod_results,
                          (tibble_row(model_name = "full additive",
                                      model = results_add$model,
                                      loocv_rmse = results_add$loocv_rmse,
                                      adj_r2 = results_add$adj_r2,
                                      sw_value = results_add$sw_value,
                                      sw_decision = results_add$sw_decision,
                                      bp_value = results_add$bp_value,
                                      bp_decision = results_add$bp_decision,
                                      num_params = results_add$num_params
                                      )))

```

```{r}
fit_log_revenue_add = lm(log(revenue) ~ .,
                     data = tmdb_movies_train)

results_log_add = get_model_results(fit_log_revenue_add)
revenue_mod_results = rbind(revenue_mod_results,
                          (tibble_row(model_name = "full log additive",
                                      model = results_log_add$model,
                                      loocv_rmse = results_log_add$loocv_rmse,
                                      adj_r2 = results_log_add$adj_r2,
                                      sw_value = results_log_add$sw_value,
                                      sw_decision = results_log_add$sw_decision,
                                      bp_value = results_log_add$bp_value,
                                      bp_decision = results_log_add$bp_decision,
                                      num_params = results_log_add$num_params
                                      )))

```

# Check VIF of the additive model

```{r}
car::vif(fit_revenue_add)

```

  The VIF of the predictors are less than 5 which is desirable

```{r}
#Find best model based on AIC using main effects
fit_revenue_add_sel = step(fit_revenue_add)

```

Based on backward search using AIC the  release_year and  is_big_banner dummy variable is getting dropped. Model search dropping the is_big_banner is somewhat surprising. We investigate using partial correlation. 


```{r}
model_add = lm(revenue ~ .-is_big_banner,  data = tmdb_movies_train )
model_predictor  = glm(is_big_banner ~ . - revenue ,  data = tmdb_movies_train, family = binomial )

cor(resid(model_add) , resid(model_predictor) )

```

 The partial correlation coefficient is very low for "is_big_banner" dummy variable, indicating the information its contributing is already captured by other predictors. Now we investigate the diagnostics of the model selected via AIC.


```{r fig.width=16, fig.height=4}
par(mar = c(5, 4, 5, 4), mfrow = c(1, 4))
plot(fit_revenue_add_sel,
    col = "grey",
    pch = 20,
    lwd = 2,
    main = "Selective Additive")
```

  
  From visual inspection the Q-Q plot and residuals vs fitted graph does not look like a good fit to the LINE  assumptions. A model thats  better fitted for normal residuals could be  found using using box cox transformations - which we attempt below.   

```{r fig.width=16, fig.height=4}
#Try to find a better fit with Box Cox methods
library(MASS)

# Find the best lambda for the response
bc = boxcox(fit_revenue_add_sel)
best.lamda = bc$x[which.max(bc$y)]


```

```{r fig.width=16, fig.height=4}

#Use the lamda found from boxcox method and do model diagnostics
fit_revenue_add_sel = lm(revenue^best.lamda  ~ budget + popularity + vote_count + release_month + first_genre,data = tmdb_movies_train)

par(mar = c(5, 4, 5, 4), mfrow = c(1, 4))
plot(fit_revenue_add_sel,
    col = "grey",
    pch = 20,
    lwd = 2,
    main = "Lambda Additive")

shapiro.test(resid(fit_revenue_add_sel))


```

```{r}
results_add_sel = get_model_results(fit_revenue_add_sel)
revenue_mod_results = rbind(revenue_mod_results,
                          (tibble_row(model_name = "Best Lambda Selective Additive",
                                      model = results_add_sel$model,
                                      loocv_rmse = results_add_sel$loocv_rmse,
                                      adj_r2 = results_add_sel$adj_r2,
                                      sw_value = results_add_sel$sw_value,
                                      sw_decision = results_add_sel$sw_decision,
                                      bp_value = results_add_sel$bp_value,
                                      bp_decision = results_add_sel$bp_decision,
                                      num_params = results_add_sel$num_params
                                      )))

```
  
The Q-Q plot looks better after box cox transformation but the normal vs residual plot indicated presence of non linearity. We now try to look at pairs plot to see presence of non linearities 

```{r fig.width=8, fig.height=8}
# Check for polynomial and interaction relationships

retain_col = c("revenue","budget", "popularity", "vote_count", "release_month", "first_genre")

pairs(tmdb_movies_train[retain_col], col = cbPalette[3])
```


From pairs plot it does seems adding  polynomial and interaction terms could get better fitting models with numeric predictors of budget, popularity and vole count. We modify the model to add interaction and polynomial terms for numeric predictors. 

Another observation from plots is the release month seems to have impact on the revenue - with movies released in summer months (April/May/June) and Nov/Dec months  having a higher revenue than other months on average. Since month is factor variable we leave it unmodified.     


```{r fig.width=16, fig.height=4}

fit_revenue_int_sel = lm(revenue^best.lamda  ~ budget + popularity + sqrt(vote_count) + release_month + first_genre +  budget*vote_count*release_month + I(budget^2) + I(popularity^2) +  I(popularity^3) + I(vote_count^2),data = tmdb_movies_train)

par(mar = c(5, 4, 5, 4), mfrow = c(1, 4))
plot(fit_revenue_int_sel,
    col = "grey",
    pch = 20,
    lwd = 2,
    main = "Interactive/Polynomial")

sum = summary(fit_revenue_int_sel)

sum$r.squared
```

```{r}
results_int_sel = get_model_results(fit_revenue_int_sel)
revenue_mod_results = rbind(revenue_mod_results,
                          (tibble_row(model_name = "Interactive/Polynomial",
                                      model = results_int_sel$model,
                                      loocv_rmse = results_int_sel$loocv_rmse,
                                      adj_r2 = results_int_sel$adj_r2,
                                      sw_value = results_int_sel$sw_value,
                                      sw_decision = results_int_sel$sw_decision,
                                      bp_value = results_int_sel$bp_value,
                                      bp_decision = results_int_sel$bp_decision,
                                      num_params = results_int_sel$num_params
                                      )))

```


```{r fig.width=16, fig.height=4}
fit_revenue_int_best = step(fit_revenue_int_sel, trace =FALSE)

par(mar = c(5, 4, 5, 4), mfrow = c(1, 4))
plot(fit_revenue_int_best,
    col = "grey",
    pch = 20,
    lwd = 2,
    main = "Interactive/Polynomial AIC")

shapiro.test(resid(fit_revenue_int_best))

sum = summary(fit_revenue_int_best)

sum$r.squared

```

```{r}
results_int_best = get_model_results(fit_revenue_int_best)
revenue_mod_results = rbind(revenue_mod_results,
                          (tibble_row(model_name = "Interactive/Polynomial AIC",
                                      model = results_int_best$model,
                                      loocv_rmse = results_int_best$loocv_rmse,
                                      adj_r2 = results_int_best$adj_r2,
                                      sw_value = results_int_best$sw_value,
                                      sw_decision = results_int_best$sw_decision,
                                      bp_value = results_int_best$bp_value,
                                      bp_decision = results_int_best$bp_decision,
                                      num_params = results_int_best$num_params
                                      )))

```



TODO: Add code chunks for other approaches and get best models for each. Add to revenue_mod_results tibble

```{r fig.width=12, fig.height=18}

#TODO: Add as many rows as best models we want to compare
par(mar = c(5, 4, 5, 4), mfrow = c(6, 4))

#TODO: Add a plot for each best model
plot(boxoffice_model_1,          
     col = "grey",
     pch = 20,
     lwd = 2,
     main = "Simple")

plot(fit_revenue_add,          
     col = "grey",
     pch = 20,
     lwd = 2,
     main = "Additive")

plot(fit_log_revenue_add,          
     col = "grey",
     pch = 20,
     lwd = 2,
     main = "Log Additive")

plot(fit_revenue_add_sel,          
     col = "grey",
     pch = 20,
     lwd = 2,
     main = "Lambda Additive")

plot(fit_revenue_int_sel,          
     col = "grey",
     pch = 20,
     lwd = 2,
     main = "Interactive/Polynomial")

plot(fit_revenue_int_best,          
     col = "grey",
     pch = 20,
     lwd = 2,
     main = "Interaction/Polynomial AIC")
```

TODO: compare the graphs for the best models we have found  

`r knitr::kable(revenue_mod_results,
              caption = "<center><strong>Revenue Model Selection</strong></center>") %>%
          kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
`

TODO: Analyze the results table and conclude on best model. Update the below chunk with the best_model's values.

```{r}
fit_revenue_best = fit_revenue_int_best
fit_revenue_best_loocv_rmse = results_int_best$loocv_rmse
fit_revenue_best_adj_r2_b = results_int_best$adj_r2
fit_revenue_best_sw_decision = results_int_best$sw_decision
fit_revenue_best_num_params = results_int_best$num_params
fit_revenue_best_preds = as.character(as.formula(fit_revenue_int_best))[3]
```

```{r}

# Run test set on the best model and calculate average percent error

predicted = predict( fit_revenue_int_best, newdata = tmdb_movies_test )
predicted_revenue = (predicted)^(1/best.lamda)
actual_revenue = tmdb_movies_test$revenue
average_percent_error = sum(abs(predicted_revenue - actual_revenue) /
                              predicted_revenue) * 100 / length(actual_revenue)

rmse = sqrt( mean( ( actual_revenue -  predicted_revenue)^2   )  )
rmse/10^6 # RMSE in millions
```

TODO: Discuss average percent error in prediction using test data

```{r fig.width=12, fig.height=4}
par(mar = c(5, 4, 5, 4), mfrow = c(1, 2))

#plot 1
plot(predicted_revenue, actual_revenue, col = "grey", pch = 20,
xlab = "Predicted", ylab = "Actual",
main = "Predicted vs Actual")

abline(0, 1, col = cbPalette[3], lwd = 2)

#plot 2
plot(density(actual_revenue), col = cbPalette[4], lwd = 2,
xlab = "Revenue", ylab = "Density",
main = "Revenue Density")
lines(density(predicted_revenue), col = cbPalette[5], lwd = 2)

legend("right",
       title = "Density",
       legend = c("Actual", "Predicted"),
       lwd = 2,
       col = c(cbPalette[4], cbPalette[5]))

```
